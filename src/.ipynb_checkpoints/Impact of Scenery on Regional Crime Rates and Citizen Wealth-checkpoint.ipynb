{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Scenery on Regional Crime Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCLAIMER:\n",
    "This is not our originally proposed project. We decided to change the topic after submitting the original proposal where we intended to create a predictive model of the impact of the expansion of trolley systems on highway rush hour traffic. We found the data necessary to make a predictive model was both sparse and would require too much time to clean. We, therefore, have changed our topic to one requiring less cleaning for a greater amount of data. Our team name remains as an homage to our dead project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Our project compares the regional crime rates to the perceived scenic value of counties in England and Wales. We found previous literature that linked scenic areas to an increase in resident happiness, happiness to a decrease in crime, and wealth to decrease crime in wealthy countries. Using these relationships, we decided to take a closer look on whether scenery affects crime rates and if so, to what extent. To determine if scenery is actually having an effect, we controlled for the effects of income with multivariate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names & Group Members IDs\n",
    "\n",
    "- Yunan Zhang： A15704964\n",
    "- Woonjoon Baek： A15745133\n",
    "- Mazen Siddiqui： A92033769\n",
    "- Mische Holland： A13803935\n",
    "- Erika Joun：  A13673598\n",
    "- Tiancheng Jiang： A14518985\n",
    "\n",
    "# Research Question\n",
    "What effect does scenery have on crime rate in England and Wales?\n",
    "\n",
    "# Background and Prior Work\n",
    "A topic that is not often explored is the effect that the aesthetics of our surroundings have on our well-being. The impact might be more than we think, as Cognitive Science shows that our brains are constantly processing visual input in ways we can't detect consciously. In urban cities, beauty is often overlooked as non-essential and is sacrificed for the sake of costs and functionality. On the contrary, studies have shown the many positive effects of having beautiful surroundings, as well as the detriments of bland and unsightly scenery:\n",
    "\n",
    " - Reported happiness is greater in more scenic locations:\n",
    "https://www.nature.com/articles/s41598-019-40854-6\n",
    "\n",
    " - Visual art in hospitals makes patients more comfortable and happier:\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5328392/\n",
    "\n",
    " - Patients in newer wards recovered faster and needed less pain medication:\n",
    "http://www.wales.nhs.uk/sites3/documents/254/ArchHealthEnv.pdf\n",
    "\n",
    "We will be expanding on this question to determine if scenery also helps to curb crime. We will be exploring this on a larger scale by comparing the scenery perceptions and crime rates of regional data, specifically counties in England and Wales. We will also be controlling for other factors such as population and income levels that may also have an effect on the crime rate.\n",
    "\n",
    "The inception of this project began with a recently published article in Nature where researchers quantified both scenery and happiness in regions of the UK and found a positive correlation between the two. (https://www.nature.com/articles/s41598-019-40854-6). To build on that work, we looked into other variables that could be affected by crime. In this social study,  https://journals.sagepub.com/doi/10.1177/1477370814536323, they found that happiness correlates inversely to crime, concluding happier people generally commit less crime. Knowing that scenery correlates to happiness which in turn correlates to crime, we aimed to do a direct comparison between scenery and crime. To add onto our analysis of crime, we wanted to look at possible correlations between wealth and scenery. In this other social study, https://ideas.repec.org/p/hcx/wpaper/0907.html, the researchers found that crime decreases as wealth increases, but only in wealthy countries. Since the UK is a wealthy country in the world and based on the various studies we looked at, we hoped to see some sort of correlation between income and scenery.\n",
    "\n",
    "While our work will only show correlation and not causation, further study into the cause for correlation could lead to exciting and novel methods of governance. A shown relation between scenery and crime rates could lead to more novel crime prevention methods such as improved scenic urban planning or increased preservation of scenic areas. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "We predict that counties in England and Wales with more beautiful scenery will have less crime, because scenic locations with happier people will be less likely to commit crime. As for what types of crime are reduced, we predict that violent crime will be reduced the most "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "For this project, we used three sets of data: scenery ratings, regional crime rates, and regional household disposable income.\n",
    "#### Scenery data: \n",
    "https://www.nature.com/articles/s41598-019-40854-6<br/>\n",
    "The scenery rating data was taken from the recently published paper relating happiness to scenery. The scenery rating data was gathered through an online survey in 2014 where users rated a series of photos for scenic value and is publicly available through the online published article. It only contains ratings for photos that have been rated 3 times or more by different people, to control for the impact of people’s personal tastes. The original data set named votes.tsv. Its shape is 212213 x 7 and contains ID, Lat, Lon, Average, Variance, Votes, and Geograph URI. This data did not have the location in counties so it was processed with the following webscraping code to follow the link and extract information listed on the photo details page. \n",
    "\n",
    "These new columns include:\n",
    "- Place - The name of the scenic site\n",
    "- Near - Which location the scenic site is near to, as described in the webpage\n",
    "- County - The county the scenic site is located in\n",
    "- Category - The category of scenic site (farmland, church, village, etc.)\n",
    "- Image - The photograph of the scenic site\n",
    "- Date - The date the photograph was taken\n",
    "\n",
    "Faithfulness of the data: There are some problems with the reliability of the data as the ratings are only dependant on one photo of an area. Because the rating website is also online, anyone can rate places even if they have never been there before. We can expect the data to be biased based on the quality of the photo representing the place. However, a good thing is that the photos were taken in small intervals of 1km of area, meaning that the photos are more likely to accurately portray the scenery of an area. \n",
    "\n",
    "Web scraping code used for obtaining the scenery data is provided below.\n",
    "\n",
    "\n",
    "#### Crime rate data:\n",
    "https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/datasets/policeforceareadatatables <br/>\n",
    "The crime rate data was taken from the UK’s Office of National Statistics and lists the counts of different types of crime in 2018 by police force area in the first tab of data. The other tabs were not considered for this project because they were not raw data values and contained some longitudinal study analysis that was not relevant to the project. We did take the population data per police force from the second tab of data in order to normalize the raw crime counts data in order to properly compared between police force areas. The shape of the data with the combined population data and raw crime counts overall and by type is 43x24.\n",
    "\n",
    "#### Income data:\n",
    "https://www.ons.gov.uk/economy/regionalaccounts/grossdisposablehouseholdincome/datasets/regionalgrossdisposablehouseholdincomegdhi <br/>\n",
    "The income data was taken from the UK’s Office of National Statistics and lists the average disposable household income in 2018 by region. We only took the first tab of data and the shape of that data is 40x2.<br/>\n",
    "\n",
    "To combine these datasets, we had to convert all regions in the income data and all counties from the scenery data into police force areas in order to compare all three variables since police force area was the most broad regional category.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscraping Code:\n",
    "```\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def storeData(url, rating, variance, id_num):\n",
    "\tpage = requests.get(url)\n",
    "\tsoup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "\n",
    "\t# Skip if page is not found\n",
    "\theader = soup(id=\"maincontent\")[0].find('h2').getText()\n",
    "\tif header == \"Sorry, page not found\":\n",
    "\t\tprint(\"Skipping id=\" + id_num)\n",
    "\t\treturn\n",
    "\n",
    "\t# Include ID\n",
    "\tprint('ID: ' + str(id_num))\n",
    "\n",
    "\t# Retrieve place name\n",
    "\tplace = soup(itemtype=\"schema.org/Photograph\")[0].find('h2').contents[1]\n",
    "\tplace = place[3:]\n",
    "\tprint('Place: ' + place)\n",
    "\n",
    "\t# Include Rating\n",
    "\tprint('Rating: ' + rating)\n",
    "\n",
    "\t# Include Variance\n",
    "\tprint('Variance: ' + variance)\n",
    "\n",
    "\t# Retrieve near area (village, city, etc.)\n",
    "\tnear = soup(itemprop=\"contentLocation\")[0].findAll('b', text=True)[-1].getText()\n",
    "\tprint('Near: ' + near)\n",
    "\n",
    "\t# Retrieve county\n",
    "\tcounty = soup(itemprop=\"contentLocation\")[0].find('i', text=True).getText()\n",
    "\tcounty = county.split(',')[1]\n",
    "\tcounty = county[1:]\n",
    "\tprint('County: ' + county)\n",
    "\n",
    "\t# Retrieve category\n",
    "\tcategory = soup(itemprop=\"keywords\") or ''\n",
    "\tif category != '':\n",
    "\t\tcategory = category[0].getText()\n",
    "\tprint(\"Category: \" + category)\n",
    "\n",
    "\t# Retrieve image\n",
    "\timage = soup(itemprop=\"contentURL\")[0]['src']\n",
    "\tprint(\"Image: \" + image)\n",
    "\n",
    "\t# Retrieve date taken\n",
    "\tdateData = soup(itemprop=\"exifData\") or soup(itemprop=\"uploadDate\")\n",
    "\tdate = dateData[0].getText()\n",
    "\tprint(\"Date: \" + date)\n",
    "\n",
    "\t# Include URL\n",
    "\tprint(\"URL: \" + url)\n",
    "\n",
    "\tfilewriter.writerow([id_num, place, rating, variance, near, county, category, image, date, url])\n",
    "\n",
    "\n",
    "# Open csv\n",
    "csvfile = open('scenery.csv', 'a', encoding='utf-8')\n",
    "filewriter = csv.writer(csvfile, lineterminator = '\\n')\n",
    "\n",
    "# Checkpoint starts the program loop starting with that id number\n",
    "df = pd.read_csv('scenery.csv', encoding='ISO-8859-1')\n",
    "if len(df) == 0:\n",
    "\tcheckpoint_id = -1\n",
    "else:\n",
    "\t#checkpoint_id = df['ID'][len(df['ID']) - 1] + 1\n",
    "        checkpoint_id = 217675\n",
    "\n",
    "# Store all urls and ratings\n",
    "if checkpoint_id == -1:\n",
    "\tfilewriter.writerow(['ID', 'Place', 'Rating', 'Variance', 'Near', 'County', 'Category', 'Image', 'Date', 'URL'])\n",
    "with open('./votes.tsv', 'r') as fp:\n",
    "\tfor count, line in enumerate(fp):\n",
    "\t\tif count != 0:\n",
    "\t\t\tdata = line.split('\\t')\n",
    "\t\t\tid_num = data[0]\n",
    "\n",
    "\t\t\tif int(id_num) >= checkpoint_id:\n",
    "\t\t\t\trating = data[4]\n",
    "\t\t\t\tvariance = data[3]\n",
    "\t\t\t\turl = data[6]\n",
    "\t\t\t\tstoreData(url, rating, variance, id_num)\n",
    "\t\t\t\tprint()\n",
    "\n",
    "fp.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Libraries used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "import statistics\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### Cleaning methods\n",
    "\n",
    "#### Scenery data \n",
    "- Since this data was created using our own webscraping code, we were able to ensure we got exactly what we needed. The only cleaning that was needed was generalizing the locations into their respective Police Force Areas (PFA) in order to be able to relate it to the crime data.\n",
    "- This was done by loading the csv into a dataframe and transforming the counties into PFA using our ```matchCounty``` function. \n",
    "\n",
    "#### Crime data \n",
    "- This dataset was very organized but was split into multiple different sheets in excel. We had to remove some rows from excel because they included general information of the dataset and would make it difficult to extract the data into a dataframe. The first tab (“numbers_crime”) was of most interest to us, since it showed the number of each offence in each Police Force Area. The other sheet of interest was the “rate1000_crime” because it included the population in each police force area and we could use this to normalize the data. We took the population data from the rate1000_crime and integrated it into our “numbers_crime” dataframe, as described by the code below.\n",
    "- We normalized the data by dividing by the population count.\n",
    "- Shortened the following column names:\n",
    "    - “Total recorded crime (excluding fraud)” -> “Total recorded crime”<br/>\n",
    "    - “Death or serious injury - unlawful driving” -> “Unlawful driving”<br/>\n",
    "\n",
    "Code for taking the population from one sheet and putting it into the dataframe for the crime dataframe:\n",
    "```\n",
    "crime_df = number_crime\n",
    "\n",
    "# Add population figures column from rate1000_crime to number_crime\n",
    "crime_df.insert(1, column='Population figures', value=rate1000_crime[rate1000_crime.columns[1]])\n",
    "\n",
    "# Clean column labels\n",
    "datafields = ['Area Name', 'Population figures',\n",
    "'Total recorded crime', 'Violence against the person', 'Homicide',\n",
    "'Violence with injury', 'Violence without injury', 'Stalking and harassment',\n",
    "'Unlawful driving', 'Sexual offences', 'Robbery',\n",
    "'Theft offences', 'Burglary', 'Residential burglary',\n",
    "'Non-residential burglary', 'Vehicle offences', 'Theft from the person',\n",
    "'Bicycle theft', 'Shoplifting', 'Other theft offences',\n",
    "'Criminal damage and arson', 'Drug offences', 'Possession of weapons offences',\n",
    "'Public order offences', 'Miscellaneous crimes']\n",
    "\n",
    "number_crime.columns = datafields\n",
    "```\n",
    "\n",
    "#### Income data \n",
    "- This dataset included the average houselhold income per county in million GBP. Similar to the scenery dataset, we had to convert it into the Police Force Area. We did this once again by using our ```matchCounty``` function. \n",
    "- Dropped data for 2016, only keeping the data for 2017 so that it matches with all our other datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up our Police Force Area dataframe, which categorizes subcounties into their respective Police Force Areas. We check how many unique counties are before and after running our ```matchCounty``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 212155\n"
     ]
    }
   ],
   "source": [
    "#read the pfac data, which shows the actuall police control area of the county\n",
    "pfac = pd.read_csv('../data/PFAC.csv')\n",
    "\n",
    "#read the scenery data \n",
    "df = pd.read_csv('scenery.csv', encoding = \"utf-8-sig\")\n",
    "print('Size of the dataset: ' + str(df.shape[0]))\n",
    "# Rating and Variance are mislabeled, switch them around\n",
    "df.rename(columns={'Variance': 'Rating', 'Rating': 'Variance'}, inplace=True)\n",
    "\n",
    "#switch the colomns and rows for pfac data\n",
    "pfac = pfac.transpose()\n",
    "pfac.reset_index(inplace = True)\n",
    "\n",
    "#reset the header of the pfac data\n",
    "new_header = pfac.iloc[0]\n",
    "pfac = pfac[1:]\n",
    "pfac.columns = new_header\n",
    "crime_df = pd.read_csv('../data/number_crime.csv')\n",
    "\n",
    "#make list of crime data's column names\n",
    "cols = list(crime_df)\n",
    "\n",
    "#remove all commas from the numbers\n",
    "crime_df[cols] = crime_df[cols].replace(',', '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the county\n",
    "def matchCounty(area):\n",
    "    for i in pfac.columns:\n",
    "        if area in list(pfac[i]):\n",
    "            return i\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counties before cleaning: 214\n"
     ]
    }
   ],
   "source": [
    "#the unique\n",
    "print('Unique counties before cleaning: ' + str(len(df['County'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['County'] = df['County'].apply(matchCounty)\n",
    "print('Unique counties after cleaning: ' + str(len(df['County'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plot:\n",
    "- A bar plot was used to see any outliers in the rating data and see how they relate to another. It showed that the ratings were generally consistent across regions. Our standard deviation was used as the error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rate = df.drop(columns=['ID', 'Place', 'Variance', 'Near', 'Category', 'Image', 'Date', 'URL'])\n",
    "df_rate = df_rate.dropna()\n",
    "df_rate['Avg_Rating'] = df_rate['Rating']\n",
    "df_rate['Standard_Deviation'] = df_rate['Rating']\n",
    "df_rate = df_rate.drop(columns=['Rating'])\n",
    "df_rate['num_pictures'] = 1\n",
    "\n",
    "\n",
    "aggregation_functions = {'Avg_Rating': 'mean', 'Standard_Deviation': 'std', 'num_pictures': 'sum'}\n",
    "df_rate_mean = df_rate.groupby(df_rate['County']).aggregate(aggregation_functions)\n",
    "df_rate_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = df_rate_mean['Avg_Rating'].plot(figsize=(15, 5),kind='bar',yerr=df_rate_mean['Standard_Deviation'])\n",
    "print(plot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Does aesthetic scenery appear to reduce crime?\n",
    "\n",
    "In order to answer this question we need to see how crime and scenery relate. This is accomplished using a correlation and a scatter matrix.\n",
    "\n",
    "### Correlation matrix\n",
    "\n",
    "- A correlation matrix allowed us to see the correlation between the various different types of crime. This helped us decide which types of crime to look at in more depth in our linear and multivariable models. Theft from the person correlated very highly with scenery and robbery correlated very highly with income.\n",
    "\n",
    "### Scatter Matrices:\n",
    "\n",
    "- A scatter matrix was done to see relations among the various different types of crime related to each other. By looking at the scatter matrix, theft from the person had the highest correlation with scenery, showing that as scenery rating goes up, theft offences decrease. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime = pd.read_csv('../data/crime_final.csv')\n",
    "df_crime.drop(df_crime.columns[0],axis=1,inplace = True)\n",
    "\n",
    "#add the average scenery rating to the crime dataframe as a column\n",
    "t = df_rate_mean['Avg_Rating']\n",
    "df_crime['avg_scenery'] = t\n",
    "\n",
    "l = df_rate_mean.index.tolist()\n",
    "for i in range(0,41):\n",
    "    \n",
    "    #match the police force area name in scenery dataframe to the PFA name in crime dataframe\n",
    "    if df_crime.loc[i]['AreaName'] in l:     \n",
    "        df_crime.set_value(i,'avg_scenery', df_rate_mean.loc[df_crime.loc[i]['AreaName']]['Avg_Rating'])\n",
    "        \n",
    "        \n",
    "\n",
    "df_crime = df_crime.dropna()\n",
    "\n",
    "#remove the commas in the dataset for convertion to int values\n",
    "for i in range(0,41):\n",
    "     for j in df_crime.columns:\n",
    "        if ',' in str(df_crime.loc[i][j]):\n",
    "             df_crime.set_value(i, j, df_crime.loc[i][j].replace(',' , ''))\n",
    "\n",
    "#set values as type float\n",
    "for i in df_crime.columns:\n",
    "     if i != 'AreaName':\n",
    "        df_crime[i] = df_crime[i].astype(float)\n",
    "\n",
    "#normalize the crime counts by population in the region\n",
    "df_crime_norm = pd.DataFrame()\n",
    "df_crime_norm['AreaName'] = df_crime['AreaName']\n",
    "for i in df_crime.columns:\n",
    "    #normalize all columns except 'PopulationFigures', 'avg_scenery', 'HouseHoldfigures'\n",
    "    if (i != 'AreaName') & (i != 'PopulationFigures') & (i != 'avg_scenery') & (i != 'HouseHoldfigures'):\n",
    "        df_crime_norm[i] = df_crime[i] / df_crime['PopulationFigures']\n",
    "        \n",
    "df_crime_norm['avg_scenery'] = df_crime['avg_scenery']\n",
    "df_crime_norm.head()\n",
    "\n",
    "#drop outlier(London metropolitan area)\n",
    "df_crime_norm.drop(index=27,inplace = True)\n",
    "df_crime_norm.reset_index(drop=True).head()\n",
    "\n",
    "#find correlation between columns\n",
    "df_crime_norm.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot scatter matrix of selected columns with high correlation\n",
    "pd.plotting.scatter_matrix(df_crime_norm.loc[:,['TotalRecordedCrime','avg_scenery','TheftFromThePerson','TheftOffences','VehicleOffences','ResidentialBurglary']],alpha = 0.9, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial:\n",
    "\n",
    "- Geospatial analyses were done to help visualize how our various variables relate along Police Force Areas. We looked at the scenery, crime, and income for each Police Force Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = gpd.read_file('../data/PFA Geospatial Analysis Stuff/Police_Force_Areas_December_2016_Full_Extent_Boundaries_in_England_and_Wales.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime['TotalCrimeNorm'] = df_crime['TotalRecordedCrime']/df_crime['PopulationFigures']\n",
    "df_crime.drop(index=27,inplace = True)\n",
    "merged = map_df.set_index(\"pfa16nm\").join(df_crime.set_index(\"AreaName\"))\n",
    "merged = merged.join(df_rate_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a variable that will call whatever column we want to visualise on the map\n",
    "variable = 'TotalCrimeNorm'\n",
    "\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = float(min(merged[variable])), float(max(merged[variable]))\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(10, 15))\n",
    "\n",
    "#labeling\n",
    "map_df['coords'] = map_df['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "map_df['coords'] = [coords[0] for coords in map_df['coords']]\n",
    "base = map_df.plot(ax = ax)\n",
    "for idx, row in map_df.iterrows():\n",
    "    plt.annotate(s=row['objectid'], xy=row['coords'], horizontalalignment='center', color = 'black')\n",
    "\n",
    "# create map\n",
    "cmap = plt.get_cmap('Blues')\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "merged.plot(column = variable, cmap = new_cmap, linewidth = 0.8, ax = base, edgecolor = '0.8')\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap=new_cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "\n",
    "# add the colorbar to the figure\n",
    "divider = make_axes_locatable(ax)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar = fig.colorbar(sm, cax = cax1)\n",
    "\n",
    "# remove the axis\n",
    "ax.axis('off')\n",
    "\n",
    "# add a title and y label\n",
    "title = ax.set_title(\"Total Recorded Crime (excl. fraud)\", fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "y_axis = ax.annotate(\"Count\", xy = (0.97, 0.66), xycoords = 'figure fraction', rotation = 'vertical',\n",
    "                    fontsize = 17)\n",
    "\n",
    "# create an annotation for the data source\n",
    "ax.annotate('Source: Happiness is Greater in More Scenic Locations, 2019', xy = (0, 0), xycoords = 'axes fraction', \n",
    "            horizontalalignment = 'left', verticalalignment = 'top', fontsize = 12, color = '#555555')\n",
    "\n",
    "# legend box for county labels\n",
    "leg = pd.DataFrame()\n",
    "leg['Police Force Area'] = map_df['objectid'].map(str) + ' = ' + map_df['pfa16nm']\n",
    "ax.annotate(leg.to_string(formatters={'Police Force Area':'{{:<{}s}}'.format(leg['Police Force Area'].str.len().max()).format},\n",
    "                          index=False, header = None), \n",
    "            xy = (0.01,0.2), xycoords = 'figure fraction', fontsize = 9, color = 'black', horizontalalignment = 'left',\n",
    "            bbox=dict(facecolor='none', edgecolor='silver'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set a variable that will call whatever column we want to visualise on the map\n",
    "variable = 'Avg_Rating'\n",
    "\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = float(min(merged[variable])), float(max(merged[variable]))\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(10, 15))\n",
    "\n",
    "#labeling\n",
    "map_df['coords'] = map_df['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "map_df['coords'] = [coords[0] for coords in map_df['coords']]\n",
    "base = map_df.plot(ax = ax)\n",
    "for idx, row in map_df.iterrows():\n",
    "    plt.annotate(s=row['objectid'], xy=row['coords'], horizontalalignment='center', color = 'black')\n",
    "\n",
    "cmap_2 = plt.get_cmap('Greens')\n",
    "new_cmap_2 = truncate_colormap(cmap_2, 0.2, 1)\n",
    "merged.plot(column = variable, cmap = new_cmap_2, linewidth = 0.8, ax = base, edgecolor = '0.8')\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap=new_cmap_2, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "\n",
    "# add the colorbar to the figure\n",
    "divider = make_axes_locatable(ax)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar = fig.colorbar(sm, cax = cax1)\n",
    "\n",
    "# remove the axis\n",
    "ax.axis('off')\n",
    "\n",
    "# add a title and y label\n",
    "title = ax.set_title(\"Average Rating\", fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "y_axis = ax.annotate(\"Average rating out of 5\", xy = (0.97, 0.66), xycoords = 'figure fraction', rotation = 'vertical',\n",
    "                    fontsize = 17)\n",
    "\n",
    "# create an annotation for the data source\n",
    "ax.annotate('Source: Happiness is Greater in More Scenic Locations, 2019', xy = (0, 0), xycoords = 'axes fraction', \n",
    "            horizontalalignment = 'left', verticalalignment = 'top', fontsize = 12, color = '#555555')\n",
    "\n",
    "# legend box for county labels\n",
    "leg = pd.DataFrame()\n",
    "leg['Police Force Area'] = map_df['objectid'].map(str) + ' = ' + map_df['pfa16nm']\n",
    "ax.annotate(leg.to_string(formatters={'Police Force Area':'{{:<{}s}}'.format(leg['Police Force Area'].str.len().max()).format},\n",
    "                          index=False, header = None), \n",
    "            xy = (0.01,0.2), xycoords = 'figure fraction', fontsize = 9, color = 'black', horizontalalignment = 'left',\n",
    "            bbox=dict(facecolor='none', edgecolor='silver'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comment\n",
    "drop every column except 'County' and 'Rating'\n",
    "\n",
    "calculate avg_rating and standard_deviation from 'Rating' and add 'num_pictures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS & RESULTS:\n",
    "### Linear model and correlation\n",
    "We chose to stick to linear model analysis because when visualizing the data via scatter matrices, the points were either linearly correlated or random rather than any other type of curve or non-linear correlation.<br/>\n",
    "In the correlation matrix, the variables with the highest correlation value with average scenery values are \"TotalRecordedCrime\", \"TheftFromThePerson\", \"TheftOffences\", \"VehicleOffences\", \"ResidentialBurglary\",\"SexualOffences\". We do a linear regression with patsy on these columns with average scenery. Results are shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "import statistics\n",
    "\n",
    "outcome_0, predictors_0 = patsy.dmatrices(\"TotalRecordedCrime ~ avg_scenery\", data = df_crime_norm)\n",
    "mod_0 = sm.OLS(outcome_0, predictors_0)\n",
    "res_0 = mod_0.fit()\n",
    "print(res_0.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='TotalRecordedCrime',style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_1, predictors_1 = patsy.dmatrices(\"TheftFromThePerson ~ avg_scenery\", data = df_crime_norm)\n",
    "mod_1 = sm.OLS(outcome_1, predictors_1)\n",
    "res_1 = mod_1.fit()\n",
    "print(res_1.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='TheftFromThePerson',style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_2, predictors_2 = patsy.dmatrices(\"TheftOffences ~ avg_scenery\", data = df_crime_norm)\n",
    "mod_2 = sm.OLS(outcome_2, predictors_2)\n",
    "res_2 = mod_2.fit()\n",
    "print(res_2.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='TheftOffences',style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_3, predictors_3 = patsy.dmatrices(\"VehicleOffences ~ avg_scenery\", data = df_crime_norm)\n",
    "mod_3 = sm.OLS(outcome_3, predictors_3)\n",
    "res_3 = mod_3.fit()\n",
    "print(res_3.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='VehicleOffences',style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_4, predictors_4 = patsy.dmatrices(\"ResidentialBurglary ~ avg_scenery \", data = df_crime_norm)\n",
    "mod_4 = sm.OLS(outcome_4, predictors_4)\n",
    "res_4 = mod_4.fit()\n",
    "print(res_4.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='ResidentialBurglary',style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_5, predictors_5 = patsy.dmatrices(\"UnlawfulDriving ~ avg_scenery\", data = df_crime_norm)\n",
    "mod_5 = sm.OLS(outcome_5, predictors_5)\n",
    "res_5 = mod_5.fit()\n",
    "print(res_5.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='UnlawfulDriving',style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_6, predictors_6 = patsy.dmatrices(\"SexualOffences ~ avg_scenery\", data = df_crime_norm)\n",
    "mod_6 = sm.OLS(outcome_6, predictors_6)\n",
    "res_6 = mod_6.fit()\n",
    "print(res_6.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='SexualOffences',style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Does scenery have more of an effect on crime than income?\n",
    "\n",
    "Differences in levels of crime can be attributed to other factors. A main factor that is known to have a strong correlation with crime is income level of an area. We want to make sure that our results are really from scenery rather than income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_income_table1: Total GDHI at current basic prices\n",
    "income_data_file = '../data/income_table1mod.csv'\n",
    "df_income = pd.read_csv(income_data_file)\n",
    "df_income = df_income.drop(columns=['2016'])\n",
    "\n",
    "#read the pfac data, which shows the actuall police control area of the county\n",
    "pfac2 = pd.read_csv('../data/PFAC 2.csv')\n",
    "\n",
    "#switch the colomns and rows for pfac data\n",
    "pfac2 = pfac2.transpose()\n",
    "pfac2.reset_index(inplace = True)\n",
    "\n",
    "#reset the header of the pfac data\n",
    "new_header = pfac2.iloc[0]\n",
    "pfac2 = pfac2[1:]\n",
    "pfac2.columns = new_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the county\n",
    "def matchCounty(area):\n",
    "    for i in pfac2.columns:\n",
    "        if area in list(pfac2[i]):\n",
    "            return i\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'Region name' to 'Police force area'\n",
    "df_income['Region name'] = df_income['Region name'].apply(matchCounty)\n",
    "df_income['Region name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'Region name' column to 'County'\n",
    "df_income['County'] = df_income['Region name']\n",
    "df_income = df_income.drop(columns=['Region name'])\n",
    "\n",
    "# sum all the same County\n",
    "aggregation_functions = {'2017': 'mean'}\n",
    "df_income = df_income.groupby(df_income['County']).aggregate(aggregation_functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income.drop(index='City of London',inplace = True)\n",
    "df_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_norm['avg_income'] = df_income.values\n",
    "#df_crime['avg_scenery'] = t\n",
    "\n",
    "l = df_income.index.tolist()\n",
    "for i in df_crime_norm.index.tolist():\n",
    "   \n",
    "    if df_crime_norm.loc[i]['AreaName'] in l:     \n",
    "        df_crime_norm.set_value(i,'avg_income', df_income.loc[df_crime_norm.loc[i]['AreaName']]['2017'])\n",
    "\n",
    "        \n",
    "df_crime_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#show correlation matrix after adding average income as a column\n",
    "df_crime_norm.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = map_df.set_index(\"pfa16nm\").join(df_crime.set_index(\"AreaName\"))\n",
    "merged = merged.join(df_income)\n",
    "\n",
    "# set a variable that will call whatever column we want to visualise on the map\n",
    "variable = '2017'\n",
    "\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = float(min(merged[variable])), float(max(merged[variable]))\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(10, 15))\n",
    "\n",
    "#labeling\n",
    "map_df['coords'] = map_df['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "map_df['coords'] = [coords[0] for coords in map_df['coords']]\n",
    "base = map_df.plot(ax = ax)\n",
    "for idx, row in map_df.iterrows():\n",
    "    plt.annotate(s=row['objectid'], xy=row['coords'], horizontalalignment='center', color = 'black')\n",
    "\n",
    "cmap_2 = plt.get_cmap('Greens')\n",
    "new_cmap_2 = truncate_colormap(cmap, 0.2, 1)\n",
    "merged.plot(column = variable, cmap = new_cmap_2, linewidth = 0.8, ax = base, edgecolor = '0.8')\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap=new_cmap_2, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "\n",
    "# add the colorbar to the figure\n",
    "divider = make_axes_locatable(ax)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar = fig.colorbar(sm, cax = cax1)\n",
    "\n",
    "# remove the axis\n",
    "ax.axis('off')\n",
    "\n",
    "# add a title and y label\n",
    "title = ax.set_title(\"Average Income\", fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "y_axis = ax.annotate(\"Average Income(million pounds)\", xy = (0.97, 0.66), xycoords = 'figure fraction', rotation = 'vertical',\n",
    "                    fontsize = 17)\n",
    "\n",
    "# create an annotation for the data source\n",
    "ax.annotate('Source: Happiness is Greater in More Scenic Locations, 2019', xy = (0, 0), xycoords = 'axes fraction', \n",
    "            horizontalalignment = 'left', verticalalignment = 'top', fontsize = 12, color = '#555555')\n",
    "\n",
    "# legend box for county labels\n",
    "leg = pd.DataFrame()\n",
    "leg['Police Force Area'] = map_df['objectid'].map(str) + ' = ' + map_df['pfa16nm']\n",
    "ax.annotate(leg.to_string(formatters={'Police Force Area':'{{:<{}s}}'.format(leg['Police Force Area'].str.len().max()).format},\n",
    "                          index=False, header = None), \n",
    "            xy = (0.01,0.2), xycoords = 'figure fraction', fontsize = 9, color = 'black', horizontalalignment = 'left',\n",
    "            bbox=dict(facecolor='none', edgecolor='silver'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complaring p-value with alpha value = 0.01\n",
    "null hypothesis: two variable are linearly correlated\n",
    "if p-value is less than alpha, reject the null hypothesis\n",
    "otherwise, do not reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do linear regression on average scenery and average income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "import statistics\n",
    "\n",
    "outcome_7, predictors_7 = patsy.dmatrices(\"avg_scenery ~ avg_income\", data = df_crime_norm)\n",
    "mod_7 = sm.OLS(outcome_7, predictors_7)\n",
    "res_7 = mod_7.fit()\n",
    "print(res_7.summary())\n",
    "df_crime_norm.plot(x='avg_scenery',y='avg_income',style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robbery has the highest correlation value with average income in the correlation matrix, so do a linear regression on Robbery and avg_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_8, predictors_8 = patsy.dmatrices(\"Robbery ~ avg_income\", data = df_crime_norm)\n",
    "mod_8 = sm.OLS(outcome_8, predictors_8)\n",
    "res_8 = mod_8.fit()\n",
    "print(res_8.summary())\n",
    "df_crime_norm.plot(x='avg_income',y='Robbery',style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a linear regression on total crime and avg_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_8, predictors_8 = patsy.dmatrices(\"TotalRecordedCrime ~ avg_income\", data = df_crime_norm)\n",
    "mod_8 = sm.OLS(outcome_8, predictors_8)\n",
    "res_8 = mod_8.fit()\n",
    "print(res_8.summary())\n",
    "df_crime_norm.plot(x='TotalRecordedCrime',y='avg_income',style='o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above testing we found that robbery and theft from the person are both correlated to avg_scenery and avg_income. Since their p-value is higher than their alpha value.\n",
    "\n",
    "### Multivariable analysis\n",
    "\n",
    "We chose to do multivariable analysis for scenery and income with theft from the person along with another analysis for scenery and income with robbery. We chose these two crime categories because they were amongst the highest values in our correlation matrix. <br/>\n",
    "\n",
    "We want to found the relationship between crime rate with income and scenery data. We applied linear moddel bwetween those three variable to measure the correlation of those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_9, predictors_9 = patsy.dmatrices(\"Robbery ~ avg_income + avg_scenery\", data = df_crime_norm)\n",
    "mod_9 = sm.OLS(outcome_9, predictors_9)\n",
    "res_9 = mod_9.fit()\n",
    "print(res_9.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra visiualization of the three variables linear model analysis  \n",
    "From 2 variable linear regression analysis, we find out that TheftFromPerson and robbery are highly correlated with both scenery data and income data, so we do a 3D linear regression on avg_scenery, avg_income and these 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.linspace(min(df_crime_norm['avg_scenery']),max(df_crime_norm['avg_scenery']),10)\n",
    "y = np.linspace(min(df_crime_norm['avg_income']),max(df_crime_norm['avg_income']),10)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = 0.0039 -0.0009*X + 2.562e-08*Y\n",
    "\n",
    "\n",
    "threedee = plt.figure(figsize = (15,15)).gca(projection='3d',elev=10,azim=50)\n",
    "threedee.scatter(df_crime_norm['avg_scenery'], df_crime_norm['avg_income'], df_crime_norm['Robbery'])\n",
    "threedee.set_xlabel('Average scenery rating',fontsize = 18)\n",
    "threedee.set_ylabel('Average income(million pounds)',fontsize = 18)\n",
    "threedee.set_zlabel('Robbery rate',fontsize = 18)\n",
    "threedee.set_title('Scenery & Income vs. Robbery rate',fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "threedee.plot_surface(X,Y,Z,color='#FFFFFF',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a linear regression on theft from the person with avg_scenery and avg_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outcome_9, predictors_9 = patsy.dmatrices(\"TheftFromThePerson ~ avg_income + avg_scenery\", data = df_crime_norm)\n",
    "mod_9 = sm.OLS(outcome_9, predictors_9)\n",
    "res_9 = mod_9.fit()\n",
    "print(res_9.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.linspace(min(df_crime_norm['avg_scenery']),max(df_crime_norm['avg_scenery']),10)\n",
    "y = np.linspace(min(df_crime_norm['avg_income']),max(df_crime_norm['avg_income']),10)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = 0.0047 -0.0010*X + 1.935e-08*Y\n",
    "\n",
    "\n",
    "threedee = plt.figure(figsize = (15,15)).gca(projection='3d',elev=10,azim=50)\n",
    "threedee.scatter(df_crime_norm['avg_scenery'], df_crime_norm['avg_income'], df_crime_norm['TheftFromThePerson'])\n",
    "threedee.set_xlabel('Average scenery rating',fontsize = 18)\n",
    "threedee.set_ylabel('Average income(million pounds)',fontsize = 18)\n",
    "threedee.set_zlabel('Theft From The Person rate',fontsize = 18)\n",
    "threedee.set_title('Scenery & Income vs. Theft From The Person',fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "threedee.plot_surface(X,Y,Z,color='#FFFFFF',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - What makes for good scenery?\n",
    "If scenery does have an effect on crime, what urban policies should be adopted? What can we do to make places more scenic?\n",
    "In the next part, we will see what makes places rated more beautiful than others. First, we take the most scenic counties and plot what features they have in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Return the population of the county\n",
    "def county_population(county):\n",
    "    if len(df_crime[df_crime['AreaName'] == county]) > 0:\n",
    "        return df_crime[df_crime['AreaName'] == county].iloc[0]['PopulationFigures']\n",
    "\n",
    "# Most beautiful counties\n",
    "scdf = df_rate_mean.sort_values(by='Avg_Rating', ascending=False)\n",
    "\n",
    "def top_categories(county):\n",
    "    df[df['County'] == county]['Category'].value_counts()[:10].plot(kind='bar', title=county)\n",
    "    plt.show()\n",
    "    \n",
    "def bottom_categories(county):\n",
    "    df[df['County'] == county]['Category'].value_counts()[-10:].plot(kind='bar', title=county)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    county = scdf.index[i]\n",
    "    top_categories(county)\n",
    "    print(county + ' population: ' + str(county_population(county) or 'No data'))\n",
    "    \n",
    "# Top categories for the top 5 more beautiful counties\n",
    "#df[df['County'] == county]['Category']\n",
    "df_crime[df_crime['AreaName'] == 'North Wales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bottom_categories(county):\n",
    "    df[df['County'] == county]['Category'].value_counts()[-10:].plot(kind='bar', title=county)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(1, 5):\n",
    "    county = scdf.index[-i]\n",
    "    bottom_categories(county)\n",
    "    print(county + ' population: ' + str(county_population(county) or 'No data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nature categories have high scenic scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population_list = []\n",
    "for i in range(len(scdf)):\n",
    "    population_list.append(county_population(df_rate_mean.index[i])) \n",
    "population_list\n",
    "df_rate_mean['Population'] = population_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#population-scenery relationship\n",
    "outcome_pc, predictors_pc = patsy.dmatrices(\"Population ~ Avg_Rating\", data = df_rate_mean)\n",
    "mod_pc = sm.OLS(outcome_pc, predictors_pc)\n",
    "res_pc = mod_pc.fit()\n",
    "print(res_pc.summary())\n",
    "df_rate_mean.plot(x='Population',y='Avg_Rating',style='o', loglog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rate_mean.sort_values(by='Population', ascending=False).head()\n",
    "df_rate_mean.sort_values(by='Avg_Rating', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are outliers that suggest that scenery might be negatively correlated to the population size, most of the data points seem to not have any significant pattern. The outliers are Cumbria and North Wales in the top left and the Metropolitan Police at the bottom right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What features exist in scenic cities with high population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For urban planning\n",
    "#df.sort_values(by='Rating')\n",
    "\n",
    "df_rate = df.drop(columns=['ID', 'Place', 'Variance', 'Near', 'County', 'Image', 'Date', 'URL'])\n",
    "df_rate = df_rate.dropna()\n",
    "df_rate['Avg_Rating'] = df_rate['Rating']\n",
    "df_rate['Standard_Deviation'] = df_rate['Rating']\n",
    "df_rate = df_rate.drop(columns=['Rating'])\n",
    "df_rate['num_pictures'] = 1\n",
    "\n",
    "import re\n",
    "# clean category column\n",
    "df_rate['Category'] = df_rate['Category'].map(lambda x: re.sub(r'\\t+', '', x))\n",
    "df_rate['Category'] = df_rate['Category'].str.replace(\"\\uFFFD\", \"\\\"\")\n",
    "df_rate['Category'] = df_rate['Category'].str.replace(\"\\\"\", \"\")\n",
    "df_rate['Category'] = df_rate['Category'].str.strip()\n",
    "df_rate['Category'] = df_rate['Category'].str.lower()\n",
    "df_rate['Category'] = df_rate['Category'].map(lambda x: re.sub(r'\\s+',' ', x))\n",
    "\n",
    "aggregation_functions = {'Avg_Rating': 'mean', 'Standard_Deviation': 'std', 'num_pictures': 'sum'}\n",
    "df_rate_mean = df_rate.groupby(df_rate['Category']).aggregate(aggregation_functions)\n",
    "df_rate_mean.head()\n",
    "\n",
    "df_rate_mean.sort_values(by='Avg_Rating', ascending=False)\n",
    "sorted_df = df.sort_values(by='Rating', ascending=False)\n",
    "\n",
    "#sorted_df\n",
    "shorter_df_rate_mean = df_rate_mean[df_rate_mean['num_pictures'] > 100].sort_values(by='Avg_Rating')[:10]\n",
    "\n",
    "plot = shorter_df_rate_mean['Avg_Rating'].plot(figsize=(15, 5),kind='bar',yerr=shorter_df_rate_mean['Standard_Deviation'])\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features do people find aesthetically pleasing overall? We make a histogram of the highest rated and lowest rated features from the set of all counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_rate_mean[df_rate_mean['num_pictures'] > 100].sort_values(by='Avg_Rating', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy\n",
    "We have permission to use this data for this purpose because all the data used is publically available and two datasets are publicly available census data from the Office of National Statistics. No privacy was violated by this study because all data was anonymous, free of any identifying information outside of general region, and publically available. The scenery data may have potential bias towards people of high socioeconomic status since the ratings were collected via website which required the use of a computer or at least a smartphone. The crime rate data may have inherent bias from the police system since it is possible there are less strict reporting enforcement in some regions over others. An ethical issue that may result from this study is improper assumption of causation and subsequent changes of policies in policing and/or governance that could negatively impact citizens. For example, over policing in areas of less perceived scenery without proof of scenic causation of crime in the area or criminals moving to more scenic areas with lower crime reports to avoid prosecution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion\n",
    "### Summarize project and analysis done <br/>\n",
    "\n",
    "### Discuss results and significance(mention limitations)<br/>\n",
    "We were limited by the small sample size of our data. While there was a lot of data for each individual dataset, when we crossed it with the crime and income datasets, the number of county areas they had in common were actually very small. \n",
    "\n",
    "### Impact to society<br/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
